{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d46b527-7854-45b5-b7cb-a90624840cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading classification data...\n",
      "INFO:__main__:Loading training dataset...\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 5415/5415 [00:01<00:00, 5298.44it/s]\n",
      "INFO:__main__:Loading test usernames...\n",
      "INFO:__main__:Loading regression test data...\n",
      "INFO:__main__:Starting classification with text incorporation...\n",
      "INFO:__main__:Building classification DataFrame with text features...\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 5415/5415 [00:00<00:00, 130483.89it/s]\n",
      "INFO:__main__:Training classification model with text features...\n",
      "INFO:__main__:Model training complete.\n",
      "INFO:__main__:Classification predictions saved to classification_output.json\n",
      "INFO:__main__:Starting regression...\n",
      "INFO:__main__:Extracting posts data for regression...\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 5415/5415 [00:00<00:00, 277732.81it/s]\n",
      "INFO:__main__:Preparing regression features...\n",
      "INFO:__main__:Preparing regression features...\n",
      "INFO:__main__:Regression predictions saved to regression_output.json\n",
      "INFO:__main__:Completed both classification and regression predictions!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# =======================\n",
    "# Data Loading Functions\n",
    "# =======================\n",
    "def load_and_preprocess_data():\n",
    "    try:\n",
    "        # Load training classification data\n",
    "        logger.info(\"Loading classification data...\")\n",
    "        classification_data = pd.read_csv(\n",
    "            r'C:\\Users\\sarp2\\Desktop\\train-classification.csv', \n",
    "            delimiter=',', \n",
    "            header=None, \n",
    "            names=['username', 'label']\n",
    "        )\n",
    "\n",
    "        # Load training dataset\n",
    "        logger.info(\"Loading training dataset...\")\n",
    "        with open(r'C:/Users/sarp2/Desktop/training-dataset.jsonl', 'r', encoding='utf-8') as f:\n",
    "            training_data = [json.loads(line) for line in tqdm(f.readlines())]\n",
    "\n",
    "        # Load test usernames\n",
    "        logger.info(\"Loading test usernames...\")\n",
    "        with open(r'C:\\Users\\sarp2\\Desktop\\test-classification-round3.dat', 'r', encoding='utf-8') as f:\n",
    "            test_usernames = [line.strip().lower() for line in f.readlines()]\n",
    "\n",
    "        # Load regression test data\n",
    "        logger.info(\"Loading regression test data...\")\n",
    "        with open(r'C:\\Users\\sarp2\\Desktop\\test-regression-round3.jsonl', 'r', encoding='utf-8') as f:\n",
    "            regression_test_data = [json.loads(line) for line in f.readlines()]\n",
    "\n",
    "        return classification_data, training_data, test_usernames, regression_test_data\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# =======================\n",
    "# Classification Helpers\n",
    "# =======================\n",
    "def gather_text_for_user(record, max_captions=5):\n",
    "    \"\"\"\n",
    "    Combine biography, category name, and up to `max_captions` post captions\n",
    "    into a single text string for TF-IDF processing.\n",
    "    \"\"\"\n",
    "    profile = record.get('profile', {})\n",
    "    biography = str(profile.get('biography', ''))\n",
    "    category = str(profile.get('category_name', ''))\n",
    "\n",
    "    # Get up to max_captions from posts\n",
    "    posts = record.get('posts', [])\n",
    "    captions = []\n",
    "    for i, post in enumerate(posts):\n",
    "        if i >= max_captions:\n",
    "            break\n",
    "        caption_text = str(post.get('caption', ''))\n",
    "        captions.append(caption_text)\n",
    "\n",
    "    # Concatenate everything\n",
    "    full_text = biography + \" \" + category + \" \" + \" \".join(captions)\n",
    "    return full_text.strip()\n",
    "\n",
    "def process_user_data_for_classification(training_data, classification_data, max_captions=5):\n",
    "    \"\"\"\n",
    "    Build a DataFrame containing:\n",
    "      - username\n",
    "      - numeric features (follower_count, following_count, etc.)\n",
    "      - text_data (biography + category_name + first N captions)\n",
    "      - label (if available)\n",
    "    \"\"\"\n",
    "    logger.info(\"Building classification DataFrame with text features...\")\n",
    "\n",
    "    # Create a lookup from username -> label\n",
    "    classification_dict = dict(zip(\n",
    "        classification_data['username'].str.lower(),\n",
    "        classification_data['label']\n",
    "    ))\n",
    "\n",
    "    all_records = []\n",
    "    for record in tqdm(training_data):\n",
    "        profile = record.get('profile', {})\n",
    "        username = profile.get('username', '').strip().lower()\n",
    "        if not username:\n",
    "            continue\n",
    "\n",
    "        # Numeric/boolean features\n",
    "        follower_count = profile.get('follower_count', 0) or 0\n",
    "        following_count = profile.get('following_count', 0) or 0\n",
    "        post_count = profile.get('post_count', 0) or 0\n",
    "        is_private = profile.get('is_private', False)\n",
    "        is_business_account = profile.get('is_business_account', False)\n",
    "        is_verified = profile.get('is_verified', False)\n",
    "\n",
    "        # Text features\n",
    "        text_data = gather_text_for_user(record, max_captions=max_captions)\n",
    "\n",
    "        # Label (if exists)\n",
    "        label = classification_dict.get(username, None)\n",
    "\n",
    "        all_records.append({\n",
    "            'username': username,\n",
    "            'follower_count': float(follower_count),\n",
    "            'following_count': float(following_count),\n",
    "            'post_count': float(post_count),\n",
    "            'is_private': int(is_private),\n",
    "            'is_business_account': int(is_business_account),\n",
    "            'is_verified': int(is_verified),\n",
    "            'text_data': text_data,\n",
    "            'label': label\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(all_records)\n",
    "    return df\n",
    "\n",
    "def train_classification_model_with_text(df, random_state=42):\n",
    "    \"\"\"\n",
    "    Train a classification model that uses both numeric features and TF-IDF text features.\n",
    "    Returns:\n",
    "        pipeline (Pipeline): trained pipeline\n",
    "        label_encoder (LabelEncoder): to inverse transform predicted labels\n",
    "    \"\"\"\n",
    "    logger.info(\"Training classification model with text features...\")\n",
    "\n",
    "    # Keep only rows that have a label\n",
    "    df_train = df.dropna(subset=['label']).copy()\n",
    "    \n",
    "    # Separate features / labels\n",
    "    y = df_train['label'].values\n",
    "    X = df_train.drop(columns=['username', 'label'])\n",
    "\n",
    "    # Encode labels\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "\n",
    "    # We'll apply TF-IDF to 'text_data' and scaling to numeric features\n",
    "    text_features = ['text_data']\n",
    "    numeric_features = [\n",
    "        'follower_count',\n",
    "        'following_count',\n",
    "        'post_count',\n",
    "        'is_private',\n",
    "        'is_business_account',\n",
    "        'is_verified'\n",
    "    ]\n",
    "\n",
    "    # ColumnTransformer: TF-IDF for text_data, StandardScaler for numeric\n",
    "    from sklearn.pipeline import make_pipeline\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('tfidf', TfidfVectorizer(max_features=1000, stop_words='english'), 'text_data'),\n",
    "            ('scaler', StandardScaler(), numeric_features)\n",
    "        ],\n",
    "        remainder='drop'\n",
    "    )\n",
    "\n",
    "    # Build pipeline: preprocessor -> RandomForestClassifier\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('clf', RandomForestClassifier(n_estimators=200, random_state=random_state))\n",
    "    ])\n",
    "\n",
    "    # Fit pipeline\n",
    "    pipeline.fit(X, y_encoded)\n",
    "    logger.info(\"Model training complete.\")\n",
    "\n",
    "    return pipeline, le\n",
    "\n",
    "def predict_user_classes(pipeline, df, label_encoder):\n",
    "    \"\"\"\n",
    "    Predict classes for users in `df` using the trained pipeline.\n",
    "    Returns a dict: {username: predicted_label}\n",
    "    \"\"\"\n",
    "    # We'll drop 'username' and 'label' if present\n",
    "    X_test = df.drop(columns=['username', 'label'], errors='ignore')\n",
    "\n",
    "    # Predict\n",
    "    preds_encoded = pipeline.predict(X_test)\n",
    "    preds = label_encoder.inverse_transform(preds_encoded)\n",
    "\n",
    "    # Build dictionary of {username -> predicted_label}\n",
    "    results = {}\n",
    "    for username, pred in zip(df['username'], preds):\n",
    "        results[username] = pred\n",
    "    return results\n",
    "\n",
    "# =======================\n",
    "# Regression Helpers\n",
    "# =======================\n",
    "def extract_posts_data(training_data):\n",
    "    logger.info(\"Extracting posts data for regression...\")\n",
    "    all_posts = []\n",
    "    \n",
    "    for user_data in tqdm(training_data):\n",
    "        if 'posts' in user_data and isinstance(user_data['posts'], list):\n",
    "            for post in user_data['posts']:\n",
    "                if 'like_count' in post and post['like_count'] is not None:\n",
    "                    all_posts.append(post)\n",
    "    \n",
    "    return all_posts\n",
    "\n",
    "def prepare_regression_features(posts, tfidf=None, is_training=True):\n",
    "    logger.info(\"Preparing regression features...\")\n",
    "    \n",
    "    # Create DataFrame with basic features\n",
    "    features_df = pd.DataFrame([{\n",
    "        'caption': str(post.get('caption', '')),\n",
    "        'media_type': post.get('media_type', ''),\n",
    "        'comments_count': float(post.get('comments_count', 0)),\n",
    "    } for post in posts])\n",
    "\n",
    "    # Handle missing values\n",
    "    features_df['caption'] = features_df['caption'].fillna('')\n",
    "    features_df['media_type'] = features_df['media_type'].fillna('unknown')\n",
    "    features_df['comments_count'] = features_df['comments_count'].fillna(0)\n",
    "\n",
    "    # Create TF-IDF features for captions\n",
    "    if tfidf is None and is_training:\n",
    "        tfidf = TfidfVectorizer(max_features=100, stop_words='english')\n",
    "        tfidf_features = tfidf.fit_transform(features_df['caption'])\n",
    "    else:\n",
    "        tfidf_features = tfidf.transform(features_df['caption'])\n",
    "\n",
    "    # One-hot encode media_type\n",
    "    media_type_dummies = pd.get_dummies(features_df['media_type'], prefix='media_type')\n",
    "    \n",
    "    # Combine all features\n",
    "    final_features = pd.concat([\n",
    "        features_df[['comments_count']],\n",
    "        pd.DataFrame(tfidf_features.toarray()),\n",
    "        media_type_dummies\n",
    "    ], axis=1)\n",
    "\n",
    "    # Convert all column names to strings\n",
    "    final_features.columns = final_features.columns.astype(str)\n",
    "\n",
    "    if is_training:\n",
    "        # Return features, labels, and the fitted tfidf\n",
    "        return final_features, [float(post['like_count']) for post in posts], tfidf\n",
    "    else:\n",
    "        # Return only features, plus the tfidf object\n",
    "        return final_features, tfidf\n",
    "\n",
    "# =======================\n",
    "# Main\n",
    "# =======================\n",
    "def main():\n",
    "    try:\n",
    "        # Load data\n",
    "        classification_data, training_data, test_usernames, regression_test_data = load_and_preprocess_data()\n",
    "        \n",
    "        # === Classification Part ===\n",
    "        logger.info(\"Starting classification with text incorporation...\")\n",
    "\n",
    "        # 1. Build DataFrame for classification with text\n",
    "        classification_df = process_user_data_for_classification(\n",
    "            training_data=training_data,\n",
    "            classification_data=classification_data,\n",
    "            max_captions=5\n",
    "        )\n",
    "\n",
    "        # 2. Train the classification model\n",
    "        pipeline, label_encoder = train_classification_model_with_text(classification_df)\n",
    "\n",
    "        # 3. Predict on test usernames\n",
    "        test_df = classification_df[classification_df['username'].isin(test_usernames)].copy()\n",
    "        if not test_df.empty:\n",
    "            classification_preds = predict_user_classes(pipeline, test_df, label_encoder)\n",
    "            with open('classification_output.json', 'w', encoding='utf-8') as f:\n",
    "                json.dump(classification_preds, f, indent=4, ensure_ascii=False)\n",
    "            logger.info(\"Classification predictions saved to classification_output.json\")\n",
    "        else:\n",
    "            logger.warning(\"No matching test usernames found in classification_df.\")\n",
    "\n",
    "        # === Regression Part ===\n",
    "        logger.info(\"Starting regression...\")\n",
    "        training_posts = extract_posts_data(training_data)\n",
    "        \n",
    "        if training_posts:\n",
    "            # Prepare features and train regression model\n",
    "            X_train, y_train, tfidf = prepare_regression_features(training_posts, is_training=True)\n",
    "            reg_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "            reg_model.fit(X_train, y_train)\n",
    "\n",
    "            # Prepare test features\n",
    "            X_test, _ = prepare_regression_features(regression_test_data, tfidf=tfidf, is_training=False)\n",
    "\n",
    "            # Generate predictions\n",
    "            regression_predictions = {}\n",
    "            for i, post in enumerate(regression_test_data):\n",
    "                pred = reg_model.predict(X_test.iloc[[i]])[0]\n",
    "                regression_predictions[post['id']] = int(max(0, round(pred)))\n",
    "\n",
    "            with open('regression_output.json', 'w', encoding='utf-8') as f:\n",
    "                json.dump(regression_predictions, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "            logger.info(\"Regression predictions saved to regression_output.json\")\n",
    "            logger.info(\"Completed both classification and regression predictions!\")\n",
    "        else:\n",
    "            logger.warning(\"No valid posts found for regression training.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca054fd-5fe9-408f-a5d3-8542d2885288",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
